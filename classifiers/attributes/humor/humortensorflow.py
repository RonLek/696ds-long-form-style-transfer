# -*- coding: utf-8 -*-
"""Copy of Classifier_Humor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GJBiXXXTZ3Kk6JRuK53V7NpNMV0CjDg9
"""

import tensorflow as tf
tf.test.gpu_device_name()

pip install transformers datasets evaluate

# !pip install -q keras
# import keras

# pip install tensorflow[and-cuda]

# !pip install --upgrade tensorflow

# !pip install tensorflow==2.15.1

# import tensorflow as tf
# print(tf.__version__)

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import activations, optimizers, losses
from tensorflow.keras import layers, models
# from transformers import BertTokenizer, TFBertModel
# from transformers import AutoTokenizer, BertForSequenceClassification
from transformers import AutoTokenizer, TFBertForSequenceClassification

# text = "Replace me by any text you'd like."
# encoded_input = tokenizer(text, return_tensors='tf')
# output = model(encoded_input)

tf.debugging.set_log_device_placement(False)

import random

def read_sentences_from_file(file_path):
    with open(file_path, 'r', errors='ignore') as file:
        sentences = file.readlines()
    return sentences

def create_labeled_list(sentences, label):
    labeled_list = [(sentence.strip(), label) for sentence in sentences]
    return labeled_list


file1 = "funny_train.txt"
file2 = "romantic_train.txt"

sentences_from_file1 = read_sentences_from_file(file1)
sentences_from_file2 = read_sentences_from_file(file2)

labeled_list = []

# Randomly select sentences from both files and assign labels alternately
while sentences_from_file1 or sentences_from_file2:
    if sentences_from_file1:
        sentence = random.choice(sentences_from_file1)
        labeled_list.append((sentence.strip(), 1))  # Label for file 1
        sentences_from_file1.remove(sentence)

    if sentences_from_file2:
        sentence = random.choice(sentences_from_file2)
        labeled_list.append((sentence.strip(), 0))  # Label for file 2
        sentences_from_file2.remove(sentence)

random.shuffle(labeled_list)

# print("Labeled list with sentences from both files:")
# for sentence, label in labeled_list:
#     print(f"({label}): {sentence}")
labels = [label for _, label in labeled_list]
sentences = [sentence for sentence, _ in labeled_list]

print('labels length',len(labels))
print('sentences length',len(sentences))

x = sentences

y = labels


print("List of labels:")
print(y)
print("\nList of corresponding sentences:")
print(x)

print('y length',len(y))
print('x length',len(x))

print(type(y[0]))

# Assuming x and y are already defined as lists of sentences and labels
datalist = []  # Initialize the list for data entries

# Iterate through x and y to create dictionary entries
for sentence, label in zip(x, y):
    entry = {"label": label, "text": sentence}
    datalist.append(entry)

# Print the resulting data list
print(datalist[0])  # Print the first entry as an example
print(len(datalist))

print(type(datalist))
print(type(datalist[0]))

print(datalist)

print(datalist[10])

# Assuming 'data' is the list containing sentence-label pairs
split_ratio = 0.8  # 80% for training, 20% for testing
split_index = int(len(datalist) * split_ratio)

# Shuffle the data to ensure randomness
random.shuffle(datalist)

# Split the data into training and testing dictionaries
data_dict = {"train": datalist[:split_index], "test": datalist[split_index:]}

# Print the lengths of the training and testing data
print("Training data length:", len(data_dict["train"]))
print("Testing data length:", len(data_dict["test"]))

# Example print of the first entry in the training and testing data
print("First entry in training data:", data_dict["train"][0])
print("First entry in testing data:", data_dict["test"][0])
print("Entry in training data:", data_dict["train"][1])
print("Entry in testing data:", data_dict["test"][0])

data_dict["train"][1]

import tensorflow as tf
device_name = tf.test.gpu_device_name()
if len(device_name) > 0:
    print("Found GPU at: {}".format(device_name))
else:
    device_name = "/device:CPU:0"
    print("No GPU, using {}.".format(device_name))

# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

MAX_LEN = 512
sentence = x[0]
inputs = tokenizer(sentence, max_length=MAX_LEN, truncation=True, padding=True)

print(f'sentence: \'{sentence}\'')
print(f'input ids: {inputs["input_ids"]}')
print(f'attention mask: {inputs["attention_mask"]}')
print(sentence)

def construct_encodings(x, tokenizer, truncation=True, padding=True):
    return tokenizer(x, return_tensors='tf', truncation=True, padding=True)

encodings = construct_encodings(x, tokenizer, truncation=True, padding=True)

#tokenized input - just text

# from transformers import DataCollatorWithPadding

# data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

id2label = {0: "NOTFUNNY", 1: "FUNNY"}
label2id = {"NOTFUNNY": 0, "FUNNY": 1}

from transformers import create_optimizer
import tensorflow as tf

train_size = int(len(x) * (0.8)) #split is 80-20
batch_size = 25
num_epochs = 2
batches_per_epoch = train_size // batch_size
total_train_steps = int(batches_per_epoch * num_epochs)
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)

model = TFBertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2, id2label=id2label, label2id=label2id)

def construct_tfdataset(encodings, y=None):
    if y:
        return tf.data.Dataset.from_tensor_slices((dict(encodings),y))
    else:
        # this case is used when making predictions on unseen samples after training
        return tf.data.Dataset.from_tensor_slices(dict(encodings))

tfdataset = construct_tfdataset(encodings, y)

TEST_SPLIT = 0.2
BATCH_SIZE = 25

train_size = int(len(x) * (1-TEST_SPLIT))

tfdataset = tfdataset.shuffle(len(x))
tfdataset_train = tfdataset.take(train_size)
tfdataset_test = tfdataset.skip(train_size)



tfdataset_train = tfdataset_train.batch(BATCH_SIZE)
tfdataset_test = tfdataset_test.batch(BATCH_SIZE)

# print(tfdataset_test)

import evaluate

accuracy = evaluate.load("accuracy")

tf.debugging.set_log_device_placement(True)

# import tensorflow as tf
tf.debugging.set_log_device_placement(True)
model.compile(optimizer=optimizer, metrics=['accuracy'])
tf.debugging.set_log_device_placement(False)

# import tensorflow as tf
# print(tf.__version__)

# import keras
# print(keras.__version__)

import numpy as np

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=labels)

from transformers.keras_callbacks import KerasMetricCallback

metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tfdataset_test)

tf.debugging.set_log_device_placement(False)

model.fit(x=tfdataset_train, validation_data=tfdataset_test, epochs=2, callbacks=metric_callback)

"""TESTING"""

benchmarks = model.evaluate(tfdataset_test, return_dict=True, batch_size=BATCH_SIZE)
print(benchmarks)
# print(accuracy)

print(x[13002], y[13002])

"""Marvel's Avengers: The Kang Dynasty is still in early development, but we already have a few details about what's in store including the cast and creative team.

The film was first announced during San Diego Comic-Con in July 2022 when Marvel Studios president Kevin Feige unveiled plans for Phase Five and Phase Six of the MCU, which have been dubbed the Multiverse Saga.Following the introduction of He Who Remains on Disney+'s Loki, the upcoming Avengers project will revolve around an alternate-timeline variant of the character Kang the Conqueror, played by Jonathan Majors.The character makes his MCU debut in Ant-Man and the Wasp: Quantumania, which will be released on Feb. 17.From the cast to the release date, here's everything to know about Avengers: The Kang Dynasty.While Marvel hasn't revealed the official plot for the film, it will likely take inspiration from the comic of the same name, which features Kang the Conqueror and his son Marcus taking over the planet.So far, the only cast member confirmed for Avengers: The Kang Dynasty is Majors, who will be playing the titular character. However, because the film is touted as an Avengers team-up, there will likely be appearances from Paul Rudd (Ant-Man), Benedict Cumberbatch (Doctor Strange), Anthony Mackie (Captain America), Jeremy Renner (Hawkeye) and Mark Ruffalo (Hulk), among others.Speaking with EW, Feige discussed how Marvel decided on making Kang the next big bad in the MCU.

For years, we've always had the inkling that Kang would be an amazing follow-up to Thanos,\" he said. `He's got that equal stature in the comics, but he's a completely different villain. Mainly, that's because he's multiple villains. He's so unique from Thanos, which we really liked.`

 Shortly after Avengers: The Kang Dynasty was teased at San Diego Comic-Con, Marvel confirmed to Deadline that Shang-Chi and the Legend of the Ten Rings director Destin Daniel Cretton had been tapped to helm the film.

 During a conversation with Variety in October 2022, Majors confirmed that he has already started talking with Cretton about the script and his direction for the story.

 `We're dealing with myths: what is a 'Kang'? What is a movie? You know, what is an MCU movie? What does that mean? What's that look like?` Majors explained. `Those are the questions we are asking, but all of that works when it's grounded and really, really really, tucked into the given circumstances of what's going on between these people and what we can illuminate for ourselves as a species.`

 Jeff Loveness is slated to write the upcoming Avengers film. He also wrote the script for Ant-Man and the Wasp: Quantumania which is set to be released in February 2023.

 In June 2023, Variety reported that the Avengers: The Kang Dynasty release date has been pushed back a year to May 1, 2026. It will be followed by Avengers: Secret Wars on May 7, 2027. The two films will serve as the conclusion to Phase Six of the MCU, as stated during San Diego Comic-Con.

 By clicking Accept All Cookies, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts.
"""

text = "By clicking Accept All Cookies, you agree to the storing of cookies on your device to enhance site navigation, analyze site usage, and assist in our marketing efforts."

"""PIPELINE"""

from transformers import pipeline

classifier = pipeline("text-classification", model=model, tokenizer=tokenizer)

classifier(text)

MODEL_NAME = 'bert-base-uncased'
MAX_LEN = 20

def create_predictor(model, model_name, max_len):
  tkzr = AutoTokenizer.from_pretrained('bert-base-uncased')
  def predict_proba(text):
      x = [text]

      encodings = construct_encodings(x, tkzr)
      tfdataset = construct_tfdataset(encodings)
      tfdataset = tfdataset.batch(1)

      preds = model.predict(tfdataset).logits
      preds = activations.softmax(tf.convert_to_tensor(preds)).numpy()
      return preds[0][0], preds[0][1]

  return predict_proba

clf = create_predictor(model, MODEL_NAME, MAX_LEN)
print ('NOT FUNNY.   FUNNY.')
# print(clf('two people go ice skating in a rink , waiting for their lovers to woosh by and pick them up when they fall .'))
print(clf(text))

inputs = tokenizer(text, return_tensors="tf")
logits = model(**inputs).logits
predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])
print(text, "====>", model.config.id2label[predicted_class_id])

"""IMPLEMENTING PIPELINE"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
inputs = tokenizer(text, return_tensors="tf")

from transformers import TFAutoModelForSequenceClassification

# model = TFAutoModelForSequenceClassification.from_pretrained("stevhliu/my_awesome_model")
logits = model(**inputs).logits

predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])
model.config.id2label[predicted_class_id]

"""----------------------------------------------------------------------------------------------------------- ENDS --------------------------------------------------------------------------------------------------------------"""

# # Assuming x and y are already defined as lists of sentences and labels
# datalist = []  # Initialize the list for data entries

# # Iterate through x and y to create dictionary entries
# for sentence, label in zip(x, y):
#     entry = {"label": label, "text": sentence}
#     datalist.append(entry)

# # Print the resulting data list
# print(datalist[0])  # Print the first entry as an example
# print(len(datalist))

# print(type(datalist))
# print(type(datalist[0]))

# print(datalist)

# print(datalist[10])

# import random

# # Assuming 'data' is the list containing sentence-label pairs
# split_ratio = 0.8  # 80% for training, 20% for testing
# split_index = int(len(datalist) * split_ratio)

# # Shuffle the data to ensure randomness
# random.shuffle(datalist)

# # Split the data into training and testing dictionaries
# data_dict = {"train": datalist[:split_index], "test": datalist[split_index:]}

# # Print the lengths of the training and testing data
# print("Training data length:", len(data_dict["train"]))
# print("Testing data length:", len(data_dict["test"]))

# # Example print of the first entry in the training and testing data
# print("First entry in training data:", data_dict["train"][0])
# print("First entry in testing data:", data_dict["test"][0])

# print("Entry in training data:", data_dict["train"][1])
# print("Entry in testing data:", data_dict["test"][0])

# data_dict["train"][1]

# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# from datasets import load_dataset

# tokenized_imdb = dataset_dict.map(preprocess_function, batched=True)

# def construct_encodings(x, tkzr, max_len, trucation=True, padding=True):
#     return tkzr(x, max_length=max_len, truncation=trucation, padding=padding)

# encodings = construct_encodings(x, tkzr, max_len=MAX_LEN)

import evaluate

accuracy = evaluate.load("accuracy")

import numpy as np


def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return accuracy.compute(predictions=predictions, references=labels)

from datasets import Dataset

# Sample data list containing sentence-label pairs
datalist = [
    {'label': 0, 'text': 'a dog catching a tennis ball at sunset in a yard , playing fetch .'},
    {'label': 0, 'text': 'a man locking his bicycle to a metal pole so that he can find it when he returns .'},
    {'label': 1, 'text': 'a looker catches a motorcross bike on its side after a spill turn into a car .'},
    {'label': 0, 'text': 'two people walks across a field of snow not really enjoying the walk .'},
    {'label': 1, 'text': 'a child sits in a plastic toy car enjoying the view .'},
    # Add more entries as needed
]
data_dict = {'data': datalist}
# Convert the list of dictionaries to a Dataset object
dataset = Dataset.from_dict(data_dict)

# Print the Dataset object
print(dataset)
print(type(dataset))

# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
# def preprocess_function(examples):
#     return tokenizer(examples["text"], truncation=True)
# tokenized_data = dataset.map(preprocess_function, batched=True)

from evaluate import evaluator
import evaluate
from transformers import pipeline
from datasets import load_dataset

pipe = pipeline("text-classification", model=model, tokenizer=tokenizer)
metric = evaluate.load("accuracy")
data = load_dataset("imdb", split="test").shuffle().select(range(100))
# classifier(text)

task_evaluator = evaluator("text-classification")

results = task_evaluator.compute(model_or_pipeline=pipe, data=data, metric=metric,
                       label_mapping={"NOTFUNNY": 0, "FUNNY": 1})

print(results)

model_name = "bert-base-uncased"
def create_predictor(model, model_name, max_len):
  tkzr = BertTokenizer.from_pretrained('bert-base-uncased')
  def predict_proba(text):
      x = [text]

      encodings = construct_encodings(x, tkzr)
      tfdataset = construct_tfdataset(encodings)
      tfdataset = tfdataset.batch(1)

      preds = model.predict(tfdataset).logits
      preds = activations.softmax(tf.convert_to_tensor(preds)).numpy()
      return preds[0][0]

  return predict_proba

clf = create_predictor(model, model_name, 20)
print(clf('two dogs in love are playing together in the snow with full joy .'))

# N_EPOCHS = 2

# model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2, id2label=id2label, label2id=label2id)
# model = TFBertModel.from_pretrained("bert-base-uncased")
# model = TFBertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2, id2label=id2label, label2id=label2id)

# model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])
# model.summary()
# model.fit(tfdataset_train, batch_size=BATCH_SIZE, epochs=N_EPOCHS)